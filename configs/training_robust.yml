train_hyperparams:
  batch_size: 1               # possibly effective batch=16 via accumulation
  epochs: 5
  lr: 1e-5
  optimizer: AdamW
  weight_decay: 1e-4
  scheduler: # keep this?
    type: cosine              # or linear warmup+decay
    warmup_steps: 1000
  seeds: [42, 7, 2021, 123456, 987654321, 31415926, 27182818, 1618033, 19937, 5489]

models:
  MultiModalClassifier:
    embed_dim: 512
    num_classes: 2
    num_heads: 8
    num_layers: 3
    hidden_dim: 256
    dropout_rate: 0.0
    activation_function: "gelu"
    classifier_hidden_dims: [512, 128]
  
  CrossModalFusion:
    embed_dim: 512
    num_heads: 8
  
  BinaryClassifier:
    embedding_size: 512
  
  BinaryClassifierNoImage:
    embedding_size: 512
  
  BinaryClassifierNoText:
    embedding_size: 512